{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "QPIC HOI.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "w-Q_e-K0xNQx",
        "X8dmuOUTxe3A"
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "0f706c6c1962494193b86a158589a8bc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_9b27bfd0b4f4423a9f327328d1696c64",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_596d594bb49c4304b6575f1aa3ec176d",
              "IPY_MODEL_c6d9d10c876b44cc959895ae9028a1c6",
              "IPY_MODEL_f37d5d05d4174e0e96a77c45453ea391"
            ]
          }
        },
        "9b27bfd0b4f4423a9f327328d1696c64": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "596d594bb49c4304b6575f1aa3ec176d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_cd306768ccd940d2a56aa1f9c6f5c0ff",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "100%",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_7555b158a82a4a51ba2e55ec5b904e8d"
          }
        },
        "c6d9d10c876b44cc959895ae9028a1c6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_3714dd1b9026463789cb23394d2c1339",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 102502400,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 102502400,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_8ceb1878ba1948c08006e3f1f737f142"
          }
        },
        "f37d5d05d4174e0e96a77c45453ea391": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_10963b78d0204c34be5c8ce58c21797b",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 97.8M/97.8M [00:03&lt;00:00, 24.2MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_8999cfc02ddb49cd9a5415db2fb6c812"
          }
        },
        "cd306768ccd940d2a56aa1f9c6f5c0ff": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "7555b158a82a4a51ba2e55ec5b904e8d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "3714dd1b9026463789cb23394d2c1339": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "8ceb1878ba1948c08006e3f1f737f142": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "10963b78d0204c34be5c8ce58c21797b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "8999cfc02ddb49cd9a5415db2fb6c812": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cN-CrpPPzBE-"
      },
      "source": [
        "# **QPIC HOI TUTORIAL**\n",
        "\n",
        "Turorial for the Paper **`QPIC: Query-Based Pairwise Human-Object Interaction Detection with Image-Wide Contextual Information`** (2021), by Tamura, Masato and Ohashi, Hiroki and Yoshinaga, Tomoaki in CVPR\n",
        "\n",
        "  - Tutorial Author: [Esteve Valls Mascaro](https://github.com/Evm7/Tutorials-Computer-Vision)\n",
        "  - Repository used: https://github.com/hitachi-rd-cv/qpic/tree/main\n",
        "\n",
        "\n",
        "The original paper does not have any chapter devoted to HOI inference.\n",
        "In order to proceed to its use, just follow this simple tutorial.\n",
        "\n",
        "Take into account:\n",
        "\n",
        "*   Turn on GPU\n",
        "*   Inference, in this case, just prepared to work for a single image.\n",
        "*   Same colour means H-O interaction, which is written in text above human.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w-Q_e-K0xNQx"
      },
      "source": [
        "## Installment and preparation of Environment"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NaLxwj_BzUZB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e8741f70-f5de-4be0-bf7c-5cc1610ebbae"
      },
      "source": [
        "! git clone https://github.com/hitachi-rd-cv/qpic.git"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'qpic'...\n",
            "remote: Enumerating objects: 68, done.\u001b[K\n",
            "remote: Counting objects: 100% (68/68), done.\u001b[K\n",
            "remote: Compressing objects: 100% (49/49), done.\u001b[K\n",
            "remote: Total 68 (delta 22), reused 62 (delta 16), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (68/68), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "82GnxSCzzjGp"
      },
      "source": [
        "! pip install -q numpy"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VX0Nu4mOy7Sp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3b26791c-a96a-475b-dd36-20ee639d4098"
      },
      "source": [
        "! pip install -qr /content/qpic/requirements.txt"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[K     |████████████████████████████████| 69 kB 3.4 MB/s \n",
            "\u001b[K     |████████████████████████████████| 753.2 MB 14 kB/s \n",
            "\u001b[K     |████████████████████████████████| 6.6 MB 22.2 MB/s \n",
            "\u001b[?25h  Building wheel for pycocotools (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for panopticapi (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchtext 0.10.0 requires torch==1.9.0, but you have torch 1.5.1 which is incompatible.\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T2iBZSHc-CkX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "24e55f84-c2e4-44cf-c5f1-f086efc71253"
      },
      "source": [
        "!wget https://github.com/hitachi-rd-cv/qpic/releases/download/v1.0/qpic_resnet50_vcoco.pth"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2021-09-30 12:50:16--  https://github.com/hitachi-rd-cv/qpic/releases/download/v1.0/qpic_resnet50_vcoco.pth\n",
            "Resolving github.com (github.com)... 140.82.112.3\n",
            "Connecting to github.com (github.com)|140.82.112.3|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://github-releases.githubusercontent.com/345977575/50bd7a80-8651-11eb-976f-b9a0b3fabf29?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIWNJYAX4CSVEH53A%2F20210930%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20210930T125016Z&X-Amz-Expires=300&X-Amz-Signature=9f40ea16e4f95ccd54fbd784fe8d59d2694a6f7d9f262a2962dbeba6786dc80b&X-Amz-SignedHeaders=host&actor_id=0&key_id=0&repo_id=345977575&response-content-disposition=attachment%3B%20filename%3Dqpic_resnet50_vcoco.pth&response-content-type=application%2Foctet-stream [following]\n",
            "--2021-09-30 12:50:16--  https://github-releases.githubusercontent.com/345977575/50bd7a80-8651-11eb-976f-b9a0b3fabf29?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIWNJYAX4CSVEH53A%2F20210930%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20210930T125016Z&X-Amz-Expires=300&X-Amz-Signature=9f40ea16e4f95ccd54fbd784fe8d59d2694a6f7d9f262a2962dbeba6786dc80b&X-Amz-SignedHeaders=host&actor_id=0&key_id=0&repo_id=345977575&response-content-disposition=attachment%3B%20filename%3Dqpic_resnet50_vcoco.pth&response-content-type=application%2Foctet-stream\n",
            "Resolving github-releases.githubusercontent.com (github-releases.githubusercontent.com)... 185.199.108.154, 185.199.109.154, 185.199.110.154, ...\n",
            "Connecting to github-releases.githubusercontent.com (github-releases.githubusercontent.com)|185.199.108.154|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 498767133 (476M) [application/octet-stream]\n",
            "Saving to: ‘qpic_resnet50_vcoco.pth’\n",
            "\n",
            "qpic_resnet50_vcoco 100%[===================>] 475.66M  53.1MB/s    in 11s     \n",
            "\n",
            "2021-09-30 12:50:27 (44.0 MB/s) - ‘qpic_resnet50_vcoco.pth’ saved [498767133/498767133]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X8dmuOUTxe3A"
      },
      "source": [
        "## Development of the project"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7rSE99v-wi0N"
      },
      "source": [
        "import sys\n",
        "sys.path.append('/content/qpic')"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kzuAkZ2_d3m5"
      },
      "source": [
        "import argparse\n",
        "from pathlib import Path\n",
        "import numpy as np\n",
        "import copy\n",
        "import pickle\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "from datasets.vcoco import build as build_dataset\n",
        "from models.backbone import build_backbone\n",
        "from models.transformer import build_transformer\n",
        "import util.misc as utils\n",
        "from util.box_ops import box_cxcywh_to_xyxy, generalized_box_iou\n",
        "from util.misc import (NestedTensor, nested_tensor_from_tensor_list,\n",
        "                       accuracy, get_world_size, interpolate,\n",
        "                       is_dist_avail_and_initialized)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8PITBCZFeEDz"
      },
      "source": [
        "### Utils\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gmNLcF9lwYuI",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        },
        "outputId": "cff50f42-7bf5-403c-c585-1275307ac1de"
      },
      "source": [
        "valid_obj_ids = (1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 13,\n",
        "                     14, 15, 16, 17, 18, 19, 20, 21, 22, 23,\n",
        "                     24, 25, 27, 28, 31, 32, 33, 34, 35, 36,\n",
        "                     37, 38, 39, 40, 41, 42, 43, 44, 46, 47,\n",
        "                     48, 49, 50, 51, 52, 53, 54, 55, 56, 57,\n",
        "                     58, 59, 60, 61, 62, 63, 64, 65, 67, 70,\n",
        "                     72, 73, 74, 75, 76, 77, 78, 79, 80, 81,\n",
        "                     82, 84, 85, 86, 87, 88, 89, 90)\n",
        "\n",
        "verb_classes = ['hold_obj', 'stand', 'sit_instr', 'ride_instr', 'walk', 'look_obj', 'hit_instr', 'hit_obj',\n",
        "                'eat_obj', 'eat_instr', 'jump_instr', 'lay_instr', 'talk_on_phone_instr', 'carry_obj',\n",
        "                'throw_obj', 'catch_obj', 'cut_instr', 'cut_obj', 'run', 'work_on_computer_instr',\n",
        "                'ski_instr', 'surf_instr', 'skateboard_instr', 'smile', 'drink_instr', 'kick_obj',\n",
        "                'point_instr', 'read_obj', 'snowboard_instr']\n",
        "\n",
        "class Args():\n",
        "    def __init__(self):\n",
        "        self.backbone='resnet50'\n",
        "        self.batch_size= 2\n",
        "        self.dec_layers=6\n",
        "        self.device='cuda'\n",
        "        self.dilation= False\n",
        "        self.dim_feedforward= 2048\n",
        "        self.dropout= 0.1\n",
        "        self.enc_layers= 6\n",
        "        self.hidden_dim= 256\n",
        "        self.hoi_path= None\n",
        "        self.lr_backbone= 0\n",
        "        self.masks= False\n",
        "        self.missing_category_id=80\n",
        "        self.nheads=8\n",
        "        self.num_queries= 100\n",
        "        self.num_workers= 2\n",
        "        self.param_path= '/content/qpic_resnet50_vcoco.pth'\n",
        "        self.position_embedding='sine'\n",
        "        self.pre_norm=False\n",
        "        self.save_path= None\n",
        "        self.subject_category_id= 0\n",
        "\n",
        "args = Args()\n",
        "\n",
        "'''\n",
        "args = {'backbone': 'resnet50',\n",
        " 'batch_size': 2,\n",
        " 'dec_layers': 6,\n",
        " 'device': 'cuda',\n",
        " 'dilation': False,\n",
        " 'dim_feedforward': 2048,\n",
        " 'dropout': 0.1,\n",
        " 'enc_layers': 6,\n",
        " 'hidden_dim': 256,\n",
        " 'hoi_path': None,\n",
        " 'lr_backbone': 0,\n",
        " 'masks': False,\n",
        " 'missing_category_id': 80,\n",
        " 'nheads': 8,\n",
        " 'num_queries': 100,\n",
        " 'num_workers': 2,\n",
        " 'param_path': '/content/qpic_resnet50_vcoco.pth',\n",
        " 'position_embedding': 'sine',\n",
        " 'pre_norm': False,\n",
        " 'save_path': None,\n",
        " 'subject_category_id': 0}'''"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"\\nargs = {'backbone': 'resnet50',\\n 'batch_size': 2,\\n 'dec_layers': 6,\\n 'device': 'cuda',\\n 'dilation': False,\\n 'dim_feedforward': 2048,\\n 'dropout': 0.1,\\n 'enc_layers': 6,\\n 'hidden_dim': 256,\\n 'hoi_path': None,\\n 'lr_backbone': 0,\\n 'masks': False,\\n 'missing_category_id': 80,\\n 'nheads': 8,\\n 'num_queries': 100,\\n 'num_workers': 2,\\n 'param_path': '/content/qpic_resnet50_vcoco.pth',\\n 'position_embedding': 'sine',\\n 'pre_norm': False,\\n 'save_path': None,\\n 'subject_category_id': 0}\""
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kuwh59prd9yF"
      },
      "source": [
        "class DETRHOI(nn.Module):\n",
        "\n",
        "    def __init__(self, backbone, transformer, num_obj_classes, num_verb_classes, num_queries):\n",
        "        super().__init__()\n",
        "        self.num_queries = num_queries\n",
        "        self.transformer = transformer\n",
        "        hidden_dim = transformer.d_model\n",
        "        self.query_embed = nn.Embedding(num_queries, hidden_dim)\n",
        "        self.obj_class_embed = nn.Linear(hidden_dim, num_obj_classes + 1)\n",
        "        self.verb_class_embed = nn.Linear(hidden_dim, num_verb_classes)\n",
        "        self.sub_bbox_embed = MLP(hidden_dim, hidden_dim, 4, 3)\n",
        "        self.obj_bbox_embed = MLP(hidden_dim, hidden_dim, 4, 3)\n",
        "        self.input_proj = nn.Conv2d(backbone.num_channels, hidden_dim, kernel_size=1)\n",
        "        self.backbone = backbone\n",
        "\n",
        "    def forward(self, samples: NestedTensor):\n",
        "        if not isinstance(samples, NestedTensor):\n",
        "            samples = nested_tensor_from_tensor_list(samples)\n",
        "        features, pos = self.backbone(samples)\n",
        "\n",
        "        src, mask = features[-1].decompose()\n",
        "        assert mask is not None\n",
        "        hs = self.transformer(self.input_proj(src), mask, self.query_embed.weight, pos[-1])[0]\n",
        "\n",
        "        outputs_obj_class = self.obj_class_embed(hs)\n",
        "        outputs_verb_class = self.verb_class_embed(hs)\n",
        "        outputs_sub_coord = self.sub_bbox_embed(hs).sigmoid()\n",
        "        outputs_obj_coord = self.obj_bbox_embed(hs).sigmoid()\n",
        "        out = {'pred_obj_logits': outputs_obj_class[-1], 'pred_verb_logits': outputs_verb_class[-1],\n",
        "               'pred_sub_boxes': outputs_sub_coord[-1], 'pred_obj_boxes': outputs_obj_coord[-1]}\n",
        "        return out\n",
        "\n",
        "class MLP(nn.Module):\n",
        "    \"\"\" Very simple multi-layer perceptron (also called FFN)\"\"\"\n",
        "\n",
        "    def __init__(self, input_dim, hidden_dim, output_dim, num_layers):\n",
        "        super().__init__()\n",
        "        self.num_layers = num_layers\n",
        "        h = [hidden_dim] * (num_layers - 1)\n",
        "        self.layers = nn.ModuleList(nn.Linear(n, k) for n, k in zip([input_dim] + h, h + [output_dim]))\n",
        "\n",
        "    def forward(self, x):\n",
        "        for i, layer in enumerate(self.layers):\n",
        "            x = F.relu(layer(x)) if i < self.num_layers - 1 else layer(x)\n",
        "        return x"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TZRHA_Duv-jk"
      },
      "source": [
        "def buildModel(param_path ='/content/qpic_resnet50_vcoco.pth',  eval=True):\n",
        "  device = torch.device('cuda:0')\n",
        "  backbone = build_backbone(args)\n",
        "  transformer = build_transformer(args)\n",
        "  model = DETRHOI(backbone, transformer, len(valid_obj_ids)+1, len(verb_classes),\n",
        "                  args.num_queries)\n",
        "\n",
        "  model = model.to(device)\n",
        "  checkpoint = torch.load(param_path, map_location='cpu')\n",
        "  model.load_state_dict(checkpoint['model'])\n",
        "\n",
        "  if eval:\n",
        "    model = model.eval().to(device)\n",
        "  return model, device"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-66LK8J2stt_"
      },
      "source": [
        "from PIL import Image\n",
        "\n",
        "import torchvision.transforms as transforms\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor()\n",
        "])\n",
        "\n",
        "def loadImage(input_path = '/content/kitchen.png'):\n",
        "  image = Image.open(input_path).convert(\"RGB\")\n",
        "  return image, transform(image).to(device)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h0BC1MBVtCmu"
      },
      "source": [
        "import numpy as np\n",
        "def processOutputs(outputs, image, threshold = 0.2):\n",
        "  out_obj_logits, out_verb_logits, out_sub_boxes, out_obj_boxes = outputs['pred_obj_logits'], \\\n",
        "                                                                outputs['pred_verb_logits'], \\\n",
        "                                                                outputs['pred_sub_boxes'], \\\n",
        "                                                                outputs['pred_obj_boxes']\n",
        "\n",
        "  obj_prob = F.softmax(out_obj_logits, -1)\n",
        "  obj_scores, obj_labels = obj_prob[..., :-1].max(-1)\n",
        "  \n",
        "  ch, img_h, img_w = image.shape\n",
        "  img_w = torch.tensor(img_w)\n",
        "  img_h = torch.tensor(img_h)\n",
        "\n",
        "  verb_scores = out_verb_logits.sigmoid()\n",
        "  scale_fct = torch.stack([img_w, img_h, img_w, img_h]).to(verb_scores.device)\n",
        "  sub_boxes = box_cxcywh_to_xyxy(out_sub_boxes)\n",
        "  sub_boxes = sub_boxes * scale_fct\n",
        "  obj_boxes = box_cxcywh_to_xyxy(out_obj_boxes)\n",
        "  obj_boxes = obj_boxes * scale_fct\n",
        "\n",
        "  obj_scores = obj_scores.detach()\n",
        "  obj_labels = obj_labels.detach()\n",
        "  verb_scores = verb_scores.detach()\n",
        "  sub_boxes = sub_boxes.detach()\n",
        "  obj_boxes = obj_boxes.detach()\n",
        "\n",
        "  results = []\n",
        "  for os, ol, vs, sb, ob in zip(obj_scores, obj_labels, verb_scores, sub_boxes, obj_boxes):\n",
        "      sl = torch.full_like(ol, 0)\n",
        "      l = torch.cat((sl, ol))\n",
        "      b = torch.cat((sb, ob))\n",
        "      bboxes = [{'bbox': bbox, 'category_id': label} for bbox, label in zip(b.to('cpu').numpy(), l.to('cpu').numpy())]\n",
        "\n",
        "      hoi_scores = vs * os.unsqueeze(1)\n",
        "\n",
        "      verb_labels = torch.arange(hoi_scores.shape[1], device=device).view(1, -1).expand(\n",
        "          hoi_scores.shape[0], -1)\n",
        "      object_labels = ol.view(-1, 1).expand(-1, hoi_scores.shape[1])\n",
        "\n",
        "\n",
        "      ids = torch.arange(b.shape[0])\n",
        "\n",
        "      hois = [{'subject_id': subject_id, 'object_id': object_id, 'category_id': category_id, 'score': score} for\n",
        "              subject_id, object_id, category_id, score in zip(ids[:ids.shape[0] // 2].to('cpu').numpy(),\n",
        "                                                                ids[ids.shape[0] // 2:].to('cpu').numpy(),\n",
        "                                                                verb_labels.to('cpu').numpy(), hoi_scores.to('cpu').numpy())]\n",
        "\n",
        "      results.append({\n",
        "          'predictions': bboxes,\n",
        "          'hoi_prediction': hois\n",
        "      })\n",
        "\n",
        "  results_filtered = []\n",
        "  results = results[0]\n",
        "  for h in results['hoi_prediction']:\n",
        "    score = np.max(h['score'])\n",
        "    if score > threshold:\n",
        "      obj_id =  h['object_id']\n",
        "      sub_id =  h['subject_id']\n",
        "      index = np.argmax(h['score'])\n",
        "      dict_ = { 'category_id': h['category_id'][index],\n",
        "                'object_id': obj_id,\n",
        "                'score': score,\n",
        "                'subject_id': sub_id,\n",
        "                'object_bbox' : results['predictions'][obj_id],\n",
        "                'subject_id' : results['predictions'][sub_id],\n",
        "              }\n",
        "      results_filtered.append(dict_)\n",
        "  return results_filtered "
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aSLrx5A7udWG"
      },
      "source": [
        "import cv2\n",
        "\n",
        "def drawImage(image, results):\n",
        "  image = cv2.cvtColor(np.asarray(image), cv2.COLOR_RGB2BGR)\n",
        "  COLORS = np.random.uniform(0, 255, size=(len(valid_obj_ids)+1, 3))\n",
        "  for color_id, r in enumerate(results):\n",
        "    category_id_verb = r['category_id']\n",
        "    object_bbox = r['object_bbox']['bbox']\n",
        "    object_id = r['object_bbox']['category_id']\n",
        "    subject_bbox = r['subject_id']['bbox']\n",
        "    subject_id = 0\n",
        "\n",
        "    color = COLORS[color_id]\n",
        "    cv2.rectangle(\n",
        "        image,\n",
        "        (int(subject_bbox[0]), int(subject_bbox[1])),\n",
        "        (int(subject_bbox[2]), int(subject_bbox[3])),\n",
        "        color, 2\n",
        "    )\n",
        "\n",
        "    cv2.rectangle(\n",
        "        image,\n",
        "        (int(object_bbox[0]), int(object_bbox[1])),\n",
        "        (int(object_bbox[2]), int(object_bbox[3])),\n",
        "        color, 2\n",
        "    )\n",
        "\n",
        "    cv2.putText(image, verb_classes[category_id_verb], (int(subject_bbox[0]), int(subject_bbox[1]-5)),\n",
        "                cv2.FONT_HERSHEY_SIMPLEX, 0.8, color, 2,\n",
        "                lineType=cv2.LINE_AA)\n",
        "  return image"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b3EbmwE0xmQP"
      },
      "source": [
        "### Inference "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_4zmUw6p2q0m",
        "outputId": "285bf853-6066-4cdf-f8cd-57cc92669927"
      },
      "source": [
        "# Get an image from online\n",
        "!wget https://d2hl4mfiesch9e.cloudfront.net/surfersmag/wp-content/uploads/2018/07/Bildschirmfoto-2018-07-05-um-12.12.06.png -O image.jpg"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2021-09-30 12:51:46--  https://d2hl4mfiesch9e.cloudfront.net/surfersmag/wp-content/uploads/2018/07/Bildschirmfoto-2018-07-05-um-12.12.06.png\n",
            "Resolving d2hl4mfiesch9e.cloudfront.net (d2hl4mfiesch9e.cloudfront.net)... 13.249.90.104, 13.249.90.181, 13.249.90.134, ...\n",
            "Connecting to d2hl4mfiesch9e.cloudfront.net (d2hl4mfiesch9e.cloudfront.net)|13.249.90.104|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1418839 (1.4M) [image/png]\n",
            "Saving to: ‘image.jpg’\n",
            "\n",
            "image.jpg           100%[===================>]   1.35M  2.92MB/s    in 0.5s    \n",
            "\n",
            "2021-09-30 12:51:47 (2.92 MB/s) - ‘image.jpg’ saved [1418839/1418839]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67,
          "referenced_widgets": [
            "0f706c6c1962494193b86a158589a8bc",
            "9b27bfd0b4f4423a9f327328d1696c64",
            "596d594bb49c4304b6575f1aa3ec176d",
            "c6d9d10c876b44cc959895ae9028a1c6",
            "f37d5d05d4174e0e96a77c45453ea391",
            "cd306768ccd940d2a56aa1f9c6f5c0ff",
            "7555b158a82a4a51ba2e55ec5b904e8d",
            "3714dd1b9026463789cb23394d2c1339",
            "8ceb1878ba1948c08006e3f1f737f142",
            "10963b78d0204c34be5c8ce58c21797b",
            "8999cfc02ddb49cd9a5415db2fb6c812"
          ]
        },
        "id": "Yq64onk_w9kL",
        "outputId": "9d5e6d43-d3ac-4e31-e838-389d6769bf5d"
      },
      "source": [
        "model, device = buildModel()"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/resnet50-19c8e357.pth\" to /root/.cache/torch/checkpoints/resnet50-19c8e357.pth\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "0f706c6c1962494193b86a158589a8bc",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "  0%|          | 0.00/97.8M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k_PVprDrtCmx"
      },
      "source": [
        "device = torch.device('cuda:0')\n",
        "threshold = 0.1\n",
        "image_path = '/content/image.jpg' \n",
        "image, image_device = loadImage(image_path)\n",
        "outputs = model([image_device])\n",
        "results = processOutputs(outputs, image_device, threshold = threshold)\n",
        "image_save = drawImage(image, results)"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XnGXBa5ytCm2",
        "outputId": "ca17b79d-21af-476e-98d6-aa1a4f0a4583"
      },
      "source": [
        "cv2.imwrite(\"image_out\" +str(threshold)+\".jpg\", image_save)"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    }
  ]
}