{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "QPIC HOI.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "w-Q_e-K0xNQx",
        "X8dmuOUTxe3A"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cN-CrpPPzBE-"
      },
      "source": [
        "# **QPIC HOI TUTORIAL**\n",
        "\n",
        "Turorial for the Paper **`QPIC: Query-Based Pairwise Human-Object Interaction Detection with Image-Wide Contextual Information`** (2021), by Tamura, Masato and Ohashi, Hiroki and Yoshinaga, Tomoaki in CVPR\n",
        "\n",
        "  - Tutorial Author: Esteve Valls Mascaro\n",
        "  - Repository used: https://github.com/hitachi-rd-cv/qpic/tree/main\n",
        "\n",
        "\n",
        "The original paper does not have any chapter devoted to HOI inference.\n",
        "In order to proceed to its use, just follow this simple tutorial.\n",
        "\n",
        "Take into account:\n",
        "\n",
        "*   Turn on GPU\n",
        "*   Inference, in this case, just prepared to work for a single image.\n",
        "*   Same colour means H-O interaction, which is written in text above human.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w-Q_e-K0xNQx"
      },
      "source": [
        "## Installment and preparation of Environment"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NaLxwj_BzUZB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "54481ebc-266f-4ad9-a5b2-482b46b8266d"
      },
      "source": [
        "! git clone https://github.com/hitachi-rd-cv/qpic.git"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'qpic'...\n",
            "remote: Enumerating objects: 68, done.\u001b[K\n",
            "remote: Counting objects: 100% (68/68), done.\u001b[K\n",
            "remote: Compressing objects: 100% (49/49), done.\u001b[K\n",
            "remote: Total 68 (delta 22), reused 62 (delta 16), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (68/68), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "82GnxSCzzjGp"
      },
      "source": [
        "! pip install -q numpy"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VX0Nu4mOy7Sp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7ef42798-d4d9-4c93-f5cc-c23a9a0ce1d5"
      },
      "source": [
        "! pip install -qr /content/qpic/requirements.txt"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[K     |████████████████████████████████| 69 kB 2.2 MB/s \n",
            "\u001b[K     |████████████████████████████████| 753.2 MB 13 kB/s \n",
            "\u001b[K     |████████████████████████████████| 6.6 MB 19.7 MB/s \n",
            "\u001b[?25h  Building wheel for pycocotools (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for panopticapi (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchtext 0.10.0 requires torch==1.9.0, but you have torch 1.5.1 which is incompatible.\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T2iBZSHc-CkX"
      },
      "source": [
        "!wget https://github.com/hitachi-rd-cv/qpic/releases/download/v1.0/qpic_resnet50_vcoco.pth"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X8dmuOUTxe3A"
      },
      "source": [
        "## Development of the project"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kzuAkZ2_d3m5"
      },
      "source": [
        "import argparse\n",
        "from pathlib import Path\n",
        "import numpy as np\n",
        "import copy\n",
        "import pickle\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "from datasets.vcoco import build as build_dataset\n",
        "from models.backbone import build_backbone\n",
        "from models.transformer import build_transformer\n",
        "import util.misc as utils\n",
        "from util.box_ops import box_cxcywh_to_xyxy, generalized_box_iou\n",
        "from util.misc import (NestedTensor, nested_tensor_from_tensor_list,\n",
        "                       accuracy, get_world_size, interpolate,\n",
        "                       is_dist_avail_and_initialized)"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8PITBCZFeEDz"
      },
      "source": [
        "### Utils\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gmNLcF9lwYuI"
      },
      "source": [
        "valid_obj_ids = (1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 13,\n",
        "                     14, 15, 16, 17, 18, 19, 20, 21, 22, 23,\n",
        "                     24, 25, 27, 28, 31, 32, 33, 34, 35, 36,\n",
        "                     37, 38, 39, 40, 41, 42, 43, 44, 46, 47,\n",
        "                     48, 49, 50, 51, 52, 53, 54, 55, 56, 57,\n",
        "                     58, 59, 60, 61, 62, 63, 64, 65, 67, 70,\n",
        "                     72, 73, 74, 75, 76, 77, 78, 79, 80, 81,\n",
        "                     82, 84, 85, 86, 87, 88, 89, 90)\n",
        "\n",
        "verb_classes = ['hold_obj', 'stand', 'sit_instr', 'ride_instr', 'walk', 'look_obj', 'hit_instr', 'hit_obj',\n",
        "                'eat_obj', 'eat_instr', 'jump_instr', 'lay_instr', 'talk_on_phone_instr', 'carry_obj',\n",
        "                'throw_obj', 'catch_obj', 'cut_instr', 'cut_obj', 'run', 'work_on_computer_instr',\n",
        "                'ski_instr', 'surf_instr', 'skateboard_instr', 'smile', 'drink_instr', 'kick_obj',\n",
        "                'point_instr', 'read_obj', 'snowboard_instr']\n",
        "\n",
        "args = {'backbone': 'resnet50have',\n",
        " 'batch_size': 2,\n",
        " 'dec_layers': 6,\n",
        " 'device': 'cuda',\n",
        " 'dilation': False,\n",
        " 'dim_feedforward': 2048,\n",
        " 'dropout': 0.1,\n",
        " 'enc_layers': 6,\n",
        " 'hidden_dim': 256,\n",
        " 'hoi_path': None,\n",
        " 'lr_backbone': 0,\n",
        " 'masks': False,\n",
        " 'missing_category_id': 80,\n",
        " 'nheads': 8,\n",
        " 'num_queries': 100,\n",
        " 'num_workers': 2,\n",
        " 'param_path': '/content/qpic_resnet50_vcoco.pth',\n",
        " 'position_embedding': 'sine',\n",
        " 'pre_norm': False,\n",
        " 'save_path': None,\n",
        " 'subject_category_id': 0}"
      ],
      "execution_count": 172,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kuwh59prd9yF"
      },
      "source": [
        "class DETRHOI(nn.Module):\n",
        "\n",
        "    def __init__(self, backbone, transformer, num_obj_classes, num_verb_classes, num_queries):\n",
        "        super().__init__()\n",
        "        self.num_queries = num_queries\n",
        "        self.transformer = transformer\n",
        "        hidden_dim = transformer.d_model\n",
        "        self.query_embed = nn.Embedding(num_queries, hidden_dim)\n",
        "        self.obj_class_embed = nn.Linear(hidden_dim, num_obj_classes + 1)\n",
        "        self.verb_class_embed = nn.Linear(hidden_dim, num_verb_classes)\n",
        "        self.sub_bbox_embed = MLP(hidden_dim, hidden_dim, 4, 3)\n",
        "        self.obj_bbox_embed = MLP(hidden_dim, hidden_dim, 4, 3)\n",
        "        self.input_proj = nn.Conv2d(backbone.num_channels, hidden_dim, kernel_size=1)\n",
        "        self.backbone = backbone\n",
        "\n",
        "    def forward(self, samples: NestedTensor):\n",
        "        if not isinstance(samples, NestedTensor):\n",
        "            samples = nested_tensor_from_tensor_list(samples)\n",
        "        features, pos = self.backbone(samples)\n",
        "\n",
        "        src, mask = features[-1].decompose()\n",
        "        assert mask is not None\n",
        "        hs = self.transformer(self.input_proj(src), mask, self.query_embed.weight, pos[-1])[0]\n",
        "\n",
        "        outputs_obj_class = self.obj_class_embed(hs)\n",
        "        outputs_verb_class = self.verb_class_embed(hs)\n",
        "        outputs_sub_coord = self.sub_bbox_embed(hs).sigmoid()\n",
        "        outputs_obj_coord = self.obj_bbox_embed(hs).sigmoid()\n",
        "        out = {'pred_obj_logits': outputs_obj_class[-1], 'pred_verb_logits': outputs_verb_class[-1],\n",
        "               'pred_sub_boxes': outputs_sub_coord[-1], 'pred_obj_boxes': outputs_obj_coord[-1]}\n",
        "        return out\n",
        "\n",
        "class MLP(nn.Module):\n",
        "    \"\"\" Very simple multi-layer perceptron (also called FFN)\"\"\"\n",
        "\n",
        "    def __init__(self, input_dim, hidden_dim, output_dim, num_layers):\n",
        "        super().__init__()\n",
        "        self.num_layers = num_layers\n",
        "        h = [hidden_dim] * (num_layers - 1)\n",
        "        self.layers = nn.ModuleList(nn.Linear(n, k) for n, k in zip([input_dim] + h, h + [output_dim]))\n",
        "\n",
        "    def forward(self, x):\n",
        "        for i, layer in enumerate(self.layers):\n",
        "            x = F.relu(layer(x)) if i < self.num_layers - 1 else layer(x)\n",
        "        return x"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TZRHA_Duv-jk"
      },
      "source": [
        "def buildModel(param_path ='/content/qpic_resnet50_vcoco.pth',  eval=True):\n",
        "  device = torch.device('cuda:0')\n",
        "  backbone = build_backbone(args)\n",
        "  transformer = build_transformer(args)\n",
        "  model = DETRHOI(backbone, transformer, len(valid_obj_ids)+1, len(verb_classes),\n",
        "                  args.num_queries)\n",
        "\n",
        "  model = model.to(device)\n",
        "  checkpoint = torch.load(param_path, map_location='cpu')\n",
        "  model.load_state_dict(checkpoint['model'])\n",
        "\n",
        "  if eval:\n",
        "    model = model.eval().to(device)\n",
        "  return model"
      ],
      "execution_count": 168,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-66LK8J2stt_"
      },
      "source": [
        "from PIL import Image\n",
        "\n",
        "import torchvision.transforms as transforms\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor()\n",
        "])\n",
        "\n",
        "def loadImage(input_path = '/content/kitchen.png'):\n",
        "  image = Image.open(input_path).convert(\"RGB\")\n",
        "  return image, transform(image).to(device)"
      ],
      "execution_count": 144,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h0BC1MBVtCmu"
      },
      "source": [
        "import numpy as np\n",
        "def processOutputs(outputs, image, threshold = 0.2):\n",
        "  out_obj_logits, out_verb_logits, out_sub_boxes, out_obj_boxes = outputs['pred_obj_logits'], \\\n",
        "                                                                outputs['pred_verb_logits'], \\\n",
        "                                                                outputs['pred_sub_boxes'], \\\n",
        "                                                                outputs['pred_obj_boxes']\n",
        "\n",
        "  obj_prob = F.softmax(out_obj_logits, -1)\n",
        "  obj_scores, obj_labels = obj_prob[..., :-1].max(-1)\n",
        "  \n",
        "  ch, img_h, img_w = image.shape\n",
        "  img_w = torch.tensor(img_w)\n",
        "  img_h = torch.tensor(img_h)\n",
        "\n",
        "  verb_scores = out_verb_logits.sigmoid()\n",
        "  scale_fct = torch.stack([img_w, img_h, img_w, img_h]).to(verb_scores.device)\n",
        "  sub_boxes = box_cxcywh_to_xyxy(out_sub_boxes)\n",
        "  sub_boxes = sub_boxes * scale_fct\n",
        "  obj_boxes = box_cxcywh_to_xyxy(out_obj_boxes)\n",
        "  obj_boxes = obj_boxes * scale_fct\n",
        "\n",
        "  obj_scores = obj_scores.detach()\n",
        "  obj_labels = obj_labels.detach()\n",
        "  verb_scores = verb_scores.detach()\n",
        "  sub_boxes = sub_boxes.detach()\n",
        "  obj_boxes = obj_boxes.detach()\n",
        "\n",
        "  results = []\n",
        "  for os, ol, vs, sb, ob in zip(obj_scores, obj_labels, verb_scores, sub_boxes, obj_boxes):\n",
        "      sl = torch.full_like(ol, 0)\n",
        "      l = torch.cat((sl, ol))\n",
        "      b = torch.cat((sb, ob))\n",
        "      bboxes = [{'bbox': bbox, 'category_id': label} for bbox, label in zip(b.to('cpu').numpy(), l.to('cpu').numpy())]\n",
        "\n",
        "      hoi_scores = vs * os.unsqueeze(1)\n",
        "\n",
        "      verb_labels = torch.arange(hoi_scores.shape[1], device=device).view(1, -1).expand(\n",
        "          hoi_scores.shape[0], -1)\n",
        "      object_labels = ol.view(-1, 1).expand(-1, hoi_scores.shape[1])\n",
        "\n",
        "\n",
        "      ids = torch.arange(b.shape[0])\n",
        "\n",
        "      hois = [{'subject_id': subject_id, 'object_id': object_id, 'category_id': category_id, 'score': score} for\n",
        "              subject_id, object_id, category_id, score in zip(ids[:ids.shape[0] // 2].to('cpu').numpy(),\n",
        "                                                                ids[ids.shape[0] // 2:].to('cpu').numpy(),\n",
        "                                                                verb_labels.to('cpu').numpy(), hoi_scores.to('cpu').numpy())]\n",
        "\n",
        "      results.append({\n",
        "          'predictions': bboxes,\n",
        "          'hoi_prediction': hois\n",
        "      })\n",
        "\n",
        "  results_filtered = []\n",
        "  results = results[0]\n",
        "  for h in results['hoi_prediction']:\n",
        "    score = np.max(h['score'])\n",
        "    if score > threshold:\n",
        "      obj_id =  h['object_id']\n",
        "      sub_id =  h['subject_id']\n",
        "      index = np.argmax(h['score'])\n",
        "      dict_ = { 'category_id': h['category_id'][index],\n",
        "                'object_id': obj_id,\n",
        "                'score': score,\n",
        "                'subject_id': sub_id,\n",
        "                'object_bbox' : results['predictions'][obj_id],\n",
        "                'subject_id' : results['predictions'][sub_id],\n",
        "              }\n",
        "      results_filtered.append(dict_)\n",
        "  return results_filtered "
      ],
      "execution_count": 149,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aSLrx5A7udWG"
      },
      "source": [
        "import cv2\n",
        "\n",
        "def drawImage(image, results):\n",
        "  image = cv2.cvtColor(np.asarray(image), cv2.COLOR_RGB2BGR)\n",
        "  COLORS = np.random.uniform(0, 255, size=(len(valid_obj_ids)+1, 3))\n",
        "  for color_id, r in enumerate(results):\n",
        "    category_id_verb = r['category_id']\n",
        "    object_bbox = r['object_bbox']['bbox']\n",
        "    object_id = r['object_bbox']['category_id']\n",
        "    subject_bbox = r['subject_id']['bbox']\n",
        "    subject_id = 0have\n",
        "\n",
        "    color = COLORS[color_id]\n",
        "    cv2.rectangle(\n",
        "        image,\n",
        "        (int(subject_bbox[0]), int(subject_bbox[1])),\n",
        "        (int(subject_bbox[2]), int(subject_bbox[3])),\n",
        "        color, 2\n",
        "    )\n",
        "\n",
        "    cv2.rectangle(\n",
        "        image,\n",
        "        (int(object_bbox[0]), int(object_bbox[1])),\n",
        "        (int(object_bbox[2]), int(object_bbox[3])),\n",
        "        color, 2\n",
        "    )\n",
        "\n",
        "    cv2.putText(image, verb_classes[category_id_verb], (int(subject_bbox[0]), int(subject_bbox[1]-5)),\n",
        "                cv2.FONT_HERSHEY_SIMPLEX, 0.8, color, 2,\n",
        "                lineType=cv2.LINE_AA)\n",
        "  return image"
      ],
      "execution_count": 163,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b3EbmwE0xmQP"
      },
      "source": [
        "### Inference "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_4zmUw6p2q0m",
        "outputId": "92d909d6-fd9e-4542-910a-7ce9743f2e88"
      },
      "source": [
        "# Get an image from online\n",
        "!wget https://d2hl4mfiesch9e.cloudfront.net/surfersmag/wp-content/uploads/2018/07/Bildschirmfoto-2018-07-05-um-12.12.06.png -O image.jpg"
      ],
      "execution_count": 178,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2021-09-28 10:05:08--  https://d2hl4mfiesch9e.cloudfront.net/surfersmag/wp-content/uploads/2018/07/Bildschirmfoto-2018-07-05-um-12.12.06.png\n",
            "Resolving d2hl4mfiesch9e.cloudfront.net (d2hl4mfiesch9e.cloudfront.net)... 13.226.123.147, 13.226.123.2, 13.226.123.70, ...\n",
            "Connecting to d2hl4mfiesch9e.cloudfront.net (d2hl4mfiesch9e.cloudfront.net)|13.226.123.147|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1418839 (1.4M) [image/png]\n",
            "Saving to: ‘image.jpg’\n",
            "\n",
            "image.jpg           100%[===================>]   1.35M  1.24MB/s    in 1.1s    \n",
            "\n",
            "2021-09-28 10:05:10 (1.24 MB/s) - ‘image.jpg’ saved [1418839/1418839]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k_PVprDrtCmx"
      },
      "source": [
        "threshold = 0.1\n",
        "image_path = '/content/image.jpg' \n",
        "image, image_device = loadImage(image_path)\n",
        "outputs = model([image_device])\n",
        "results = processOutputs(outputs, image_device, threshold = threshold)\n",
        "image_save = drawImage(image, results)"
      ],
      "execution_count": 179,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XnGXBa5ytCm2",
        "outputId": "41cf1164-7d14-4855-cb36-a4ba1ccb1891"
      },
      "source": [
        "cv2.imwrite(\"image_out\" +str(threshold)+\".jpg\", image_save)"
      ],
      "execution_count": 180,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 180
        }
      ]
    }
  ]
}
